@article{chenImprovedPatchMixTransformer2024,
  title = {Improved {{Patch-Mix Transformer}} and {{Contrastive Learning Method}} for {{Sound Classification}} in {{Noisy Environments}}},
  author = {Chen, Xu and Wang, Mei and Kan, Ruixiang and Qiu, Hongbing},
  year = {2024},
  month = jan,
  journal = {Applied Sciences},
  volume = {14},
  number = {21},
  pages = {9711},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app14219711},
  urldate = {2025-06-14},
  abstract = {In urban environments, noise significantly impacts daily life and presents challenges for Environmental Sound Classification (ESC). The structural influence of urban noise on audio signals complicates feature extraction and audio classification for environmental sound classification methods. To address these challenges, this paper proposes a Contrastive Learning-based Audio Spectrogram Transformer (CL-Transformer) that incorporates a Patch-Mix mechanism and adaptive contrastive learning strategies while simultaneously improving and utilizing adaptive data augmentation techniques for model training. Firstly, a combination of data augmentation techniques is introduced to enrich environmental sounds. Then, the Patch-Mix feature fusion scheme randomly mixes patches of the enhanced and noisy spectrograms during the Transformer's patch embedding. Furthermore, a novel contrastive learning scheme is introduced to quantify loss and improve model performance, synergizing well with the Transformer model. Finally, experiments on the ESC-50 and UrbanSound8K public datasets achieved accuracies of 97.75\% and 92.95\%, respectively. To simulate the impact of noise in real urban environments, the model is evaluated using the UrbanSound8K dataset with added background noise at different signal-to-noise ratios (SNR). Experimental results demonstrate that the proposed framework performs well in noisy environments.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {contrastive learning,data augmentation,deep learning,feature fusion,transformer,urban environmental sound recognition},
  file = {/home/sjoerd/Zotero/storage/IF8PY2SP/Chen et al. - 2024 - Improved Patch-Mix Transformer and Contrastive Learning Method for Sound Classification in Noisy Env.pdf}
}

@misc{guzhovAudioCLIPExtendingCLIP2021,
  title = {{{AudioCLIP}}: {{Extending CLIP}} to {{Image}}, {{Text}} and {{Audio}}},
  shorttitle = {{{AudioCLIP}}},
  author = {Guzhov, Andrey and Raue, Federico and Hees, J{\"o}rn and Dengel, Andreas},
  year = {2021},
  month = jun,
  number = {arXiv:2106.13043},
  eprint = {2106.13043},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.13043},
  urldate = {2025-06-16},
  abstract = {In the past, the rapidly evolving field of sound classification greatly benefited from the application of methods from other domains. Today, we observe the trend to fuse domain-specific tasks and approaches together, which provides the community with new outstanding models. In this work, we present an extension of the CLIP model that handles audio in addition to text and images. Our proposed model incorporates the ESResNeXt audio-model into the CLIP framework using the AudioSet dataset. Such a combination enables the proposed model to perform bimodal and unimodal classification and querying, while keeping CLIP's ability to generalize to unseen datasets in a zero-shot inference fashion. AudioCLIP achieves new state-of-the-art results in the Environmental Sound Classification (ESC) task, out-performing other approaches by reaching accuracies of 90.07\% on the UrbanSound8K and 97.15\% on the ESC-50 datasets. Further it sets new baselines in the zero-shot ESC-task on the same datasets (68.78\% and 69.40\%, respectively). Finally, we also assess the cross-modal querying performance of the proposed model as well as the influence of full and partial training on the results. For the sake of reproducibility, our code is published.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/sjoerd/Zotero/storage/6ZHEF83U/Guzhov et al. - 2021 - AudioCLIP Extending CLIP to Image, Text and Audio.pdf;/home/sjoerd/Zotero/storage/KDPMYU4U/2106.html}
}

@misc{haubnerDeepLearningBasedJoint2022,
  title = {Deep {{Learning-Based Joint Control}} of {{Acoustic Echo Cancellation}}, {{Beamforming}} and {{Postfiltering}}},
  author = {Haubner, Thomas and Kellermann, Walter},
  year = {2022},
  month = aug,
  number = {arXiv:2203.01793},
  eprint = {2203.01793},
  primaryclass = {eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.01793},
  urldate = {2025-06-15},
  abstract = {We introduce a novel method for controlling the functionality of a hands-free speech communication device which comprises a model-based acoustic echo canceller (AEC), minimum variance distortionless response (MVDR) beamformer (BF) and spectral postfilter (PF). While the AEC removes the early echo component, the MVDR BF and PF suppress the residual echo and background noise. As key innovation, we suggest to use a single deep neural network (DNN) to jointly control the adaptation of the various algorithmic components. This allows for rapid convergence and high steady-state performance in the presence of high-level interfering double-talk. End-to-end training of the DNN using a time-domain speech extraction loss function avoids the design of individual control strategies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/sjoerd/Zotero/storage/WZJUIAAR/Haubner and Kellermann - 2022 - Deep Learning-Based Joint Control of Acoustic Echo Cancellation, Beamforming and Postfiltering.pdf}
}

@misc{morsaliFaceFastAccurate2023,
  title = {Face: {{Fast}}, {{Accurate}} and {{Context-Aware Audio Annotation}} and {{Classification}}},
  shorttitle = {Face},
  author = {Morsali, M. Mehrdad and Mohammadzade, Hoda and Shouraki, Saeed Bagheri},
  year = {2023},
  month = mar,
  number = {arXiv:2303.03666},
  eprint = {2303.03666},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.03666},
  urldate = {2025-06-15},
  abstract = {This paper presents a context-aware framework for feature selection and classification procedures to realize a fast and accurate audio event annotation and classification. The context-aware design starts with exploring feature extraction techniques to find an appropriate combination to select a set resulting in remarkable classification accuracy with minimal computational effort. The exploration for feature selection also embraces an investigation of audio Tempo representation, an advantageous feature extraction method missed by previous works in the environmental audio classification research scope. The proposed annotation method considers outlier, inlier, and hard-to-predict data samples to realize context-aware Active Learning, leading to the average accuracy of 90\% when only 15\% of data possess initial annotation. Our proposed algorithm for sound classification obtained average prediction accuracy of 98.05\% on the UrbanSound8K dataset. The notebooks containing our source codes and implementation results are available at https://github.com/gitmehrdad/FACE.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/sjoerd/Zotero/storage/Z6U3TVF4/Morsali et al. - 2023 - Face Fast, Accurate and Context-Aware Audio Annotation and Classification.pdf;/home/sjoerd/Zotero/storage/P6T9GHVF/2303.html}
}

@misc{nasiriSoundCLRContrastiveLearning2021,
  title = {{{SoundCLR}}: {{Contrastive Learning}} of {{Representations For Improved Environmental Sound Classification}}},
  shorttitle = {{{SoundCLR}}},
  author = {Nasiri, Alireza and Hu, Jianjun},
  year = {2021},
  month = mar,
  number = {arXiv:2103.01929},
  eprint = {2103.01929},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.01929},
  urldate = {2025-06-15},
  abstract = {Environmental Sound Classification (ESC) is a challenging field of research in non-speech audio processing. Most of current research in ESC focuses on designing deep models with special architectures tailored for specific audio datasets, which usually cannot exploit the intrinsic patterns in the data. However recent studies have surprisingly shown that transfer learning from models trained on ImageNet is a very effective technique in ESC. Herein, we propose SoundCLR, a supervised contrastive learning method for effective environment sound classification with state-of-the-art performance, which works by learning representations that disentangle the samples of each class from those of other classes. Our deep network models are trained by combining a contrastive loss that contributes to a better probability output by the classification layer with a cross-entropy loss on the output of the classifier layer to map the samples to their respective 1-hot encoded labels. Due to the comparatively small sizes of the available environmental sound datasets, we propose and exploit a transfer learning and strong data augmentation pipeline and apply the augmentations on both the sound signals and their log-mel spectrograms before inputting them to the model. Our experiments show that our masking based augmentation technique on the log-mel spectrograms can significantly improve the recognition performance. Our extensive benchmark experiments show that our hybrid deep network models trained with combined contrastive and cross-entropy loss achieved the state-of-the-art performance on three benchmark datasets ESC-10, ESC-50, and US8K with validation accuracies of 99.75{\textbackslash}\%, 93.4{\textbackslash}\%, and 86.49{\textbackslash}\% respectively. The ensemble version of our models also outperforms other top ensemble methods. The code is available at https://github.com/alireza-nasiri/SoundCLR.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/sjoerd/Zotero/storage/T46KMWVQ/Nasiri and Hu - 2021 - SoundCLR Contrastive Learning of Representations For Improved Environmental Sound Classification.pdf;/home/sjoerd/Zotero/storage/YQ3SM5G8/2103.html}
}

@inproceedings{paissanTinyCLAPDistillingConstrastive2024,
  title = {{{tinyCLAP}}: {{Distilling Constrastive Language-Audio Pretrained Models}}},
  shorttitle = {{{tinyCLAP}}},
  booktitle = {Interspeech 2024},
  author = {Paissan, Francesco and Farella, Elisabetta},
  year = {2024},
  month = sep,
  eprint = {2311.14517},
  primaryclass = {cs},
  pages = {1685--1689},
  doi = {10.21437/Interspeech.2024-193},
  urldate = {2025-06-14},
  abstract = {Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in the field of audio and speech processing. Its employment ranges from sound event detection to text-to-audio generation. However, one of the main limitations is the considerable amount of data required in the training process and the overall computational complexity during inference. This paper investigates how we can reduce the complexity of contrastive language-audio pre-trained models, yielding an efficient model that we call tinyCLAP. We derive an unimodal distillation loss from first principles and explore how the dimensionality of the shared, multimodal latent space can be reduced via pruning. TinyCLAP uses only 6\% of the original Microsoft CLAP parameters with a minimal reduction (less than 5\%) in zero-shot classification performance across the three sound event detection datasets on which it was tested},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/sjoerd/Zotero/storage/PAQ5EF35/Paissan and Farella - 2024 - tinyCLAP Distilling Constrastive Language-Audio Pretrained Models.pdf;/home/sjoerd/Zotero/storage/Q3CZ7SIU/2311.html}
}

@inproceedings{piczakESCDatasetEnvironmental2015,
  title = {{{ESC}}: {{Dataset}} for {{Environmental Sound Classification}}},
  shorttitle = {{{ESC}}},
  booktitle = {Proceedings of the 23rd {{ACM}} International Conference on {{Multimedia}}},
  author = {Piczak, Karol J.},
  year = {2015},
  month = oct,
  series = {{{MM}} '15},
  pages = {1015--1018},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2733373.2806390},
  urldate = {2025-06-16},
  abstract = {One of the obstacles in research activities concentrating on environmental sound classification is the scarcity of suitable and publicly available datasets. This paper tries to address that issue by presenting a new annotated collection of 2000 short clips comprising 50 classes of various common sound events, and an abundant unified compilation of 250000 unlabeled auditory excerpts extracted from recordings available through the Freesound project. The paper also provides an evaluation of human accuracy in classifying environmental sounds and compares it to the performance of selected baseline classifiers using features derived from mel-frequency cepstral coefficients and zero-crossing rate.},
  isbn = {978-1-4503-3459-4}
}

@inproceedings{salamonDatasetTaxonomyUrban2014,
  title = {A {{Dataset}} and {{Taxonomy}} for {{Urban Sound Research}}},
  booktitle = {Proceedings of the 22nd {{ACM}} International Conference on {{Multimedia}}},
  author = {Salamon, Justin and Jacoby, Christopher and Bello, Juan Pablo},
  year = {2014},
  month = nov,
  series = {{{MM}} '14},
  pages = {1041--1044},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2647868.2655045},
  urldate = {2025-06-14},
  abstract = {Automatic urban sound classification is a growing area of research with applications in multimedia retrieval and urban informatics. In this paper we identify two main barriers to research in this area - the lack of a common taxonomy and the scarceness of large, real-world, annotated data. To address these issues we present a taxonomy of urban sounds and a new dataset, UrbanSound, containing 27 hours of audio with 18.5 hours of annotated sound event occurrences across 10 sound classes. The challenges presented by the new dataset are studied through a series of experiments using a baseline classification system.},
  isbn = {978-1-4503-3063-3},
  file = {/home/sjoerd/Zotero/storage/J66F6ECA/Salamon et al. - 2014 - A Dataset and Taxonomy for Urban Sound Research.pdf}
}

@misc{shamsSSAMBASelfSupervisedAudio2024,
  title = {{{SSAMBA}}: {{Self-Supervised Audio Representation Learning}} with {{Mamba State Space Model}}},
  shorttitle = {{{SSAMBA}}},
  author = {Shams, Siavash and Dindar, Sukru Samet and Jiang, Xilin and Mesgarani, Nima},
  year = {2024},
  month = may,
  journal = {arXiv.org},
  doi = {10.1109/SLT61566.2024.10832304},
  urldate = {2025-06-14},
  abstract = {Transformers have revolutionized deep learning across various tasks, including audio representation learning, due to their powerful modeling capabilities. However, they often suffer from quadratic complexity in both GPU memory usage and computational inference time, affecting their efficiency. Recently, state space models (SSMs) like Mamba have emerged as a promising alternative, offering a more efficient approach by avoiding these complexities. Given these advantages, we explore the potential of SSM-based models in audio tasks. In this paper, we introduce Self-Supervised Audio Mamba (SSAMBA), the first self-supervised, attention-free, and SSM-based model for audio representation learning. SSAMBA leverages the bidirectional Mamba to capture complex audio patterns effectively. We incorporate a self-supervised pretraining framework that optimizes both discriminative and generative objectives, enabling the model to learn robust audio representations from large-scale, unlabeled datasets. We evaluated SSAMBA on various tasks such as audio classification, keyword spotting, and speaker identification. Our results demonstrate that SSAMBA outperforms the Self-Supervised Audio Spectrogram Transformer (SSAST) in most tasks. Notably, SSAMBA is approximately 92.7\% faster in batch inference speed and 95.4\% more memory-efficient than SSAST for the tiny model size with an input token size of 22k. These efficiency gains, combined with superior performance, underscore the effectiveness of SSAMBA's architectural innovation, making it a compelling choice for a wide range of audio processing applications.},
  howpublished = {https://arxiv.org/abs/2405.11831v2},
  langid = {english},
  file = {/home/sjoerd/Zotero/storage/2SZMUDXT/Shams et al. - 2024 - SSAMBA Self-Supervised Audio Representation Learning with Mamba State Space Model.pdf}
}
